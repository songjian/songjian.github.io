---
layout: post
title: "Ollama"
---
安装Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

本地运行Llama3

```bash
ollama run llama3
```

等待一会就可以在本地和大语言模型聊天了

```bash
>>> Send a message (/? for help)
```

Open WebUI

如果Ollama和Open WebUI在一台电脑上，执行：

```bash
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

Open WebUI的容器启动之后在浏览器访问[http://localhost:3000/](http://localhost:3000/)即可打开WebUI。
